{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saranyanv/test/blob/master/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6cyHleshQD1P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import re  # regular expressions\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "def reset_graph(seed= RANDOM_SEED):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\t\n",
        "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
        "    \"\"\"\n",
        "    Read a embeddings txt file. If `with_indexes=True`, \n",
        "    we return a tuple of two dictionnaries\n",
        "    `(word_to_index_dict, index_to_embedding_array)`, \n",
        "    otherwise we return only a direct \n",
        "    `word_to_embedding_dict` dictionnary mapping \n",
        "    from a string to a numpy array.\n",
        "    \"\"\"\n",
        "    if with_indexes:\n",
        "        word_to_index_dict = dict()\n",
        "        index_to_embedding_array = []\n",
        "  \n",
        "    else:\n",
        "        word_to_embedding_dict = dict()\n",
        "\n",
        "    with open(embeddings_filename, 'r') as embeddings_file:\n",
        "        for (i, line) in enumerate(embeddings_file):\n",
        "\n",
        "            split = line.split(' ')\n",
        "\n",
        "            word = split[0]\n",
        "\n",
        "            representation = split[1:]\n",
        "            representation = np.array(\n",
        "                [float(val) for val in representation]\n",
        "            )\n",
        "\n",
        "            if with_indexes:\n",
        "                word_to_index_dict[word] = i\n",
        "                index_to_embedding_array.append(representation)\n",
        "            else:\n",
        "                word_to_embedding_dict[word] = representation\n",
        "\n",
        "    # Empty representation for unknown words.\n",
        "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
        "    if with_indexes:\n",
        "        _LAST_INDEX = i + 1\n",
        "        word_to_index_dict = defaultdict(\n",
        "            lambda: _LAST_INDEX, word_to_index_dict)\n",
        "        index_to_embedding_array = np.array(\n",
        "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
        "        return word_to_index_dict, index_to_embedding_array\n",
        "    else:\n",
        "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
        "        return word_to_embedding_dict\n",
        "\n",
        "def default_factory():\n",
        "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
        "\n",
        "\n",
        "def listdir_no_hidden(path):\n",
        "    start_list = os.listdir(path)\n",
        "    end_list = []\n",
        "    for file in start_list:\n",
        "        if (not file.startswith('.')):\n",
        "            end_list.append(file)\n",
        "    return(end_list)\n",
        "\n",
        "def text_parse(string):\n",
        "    # replace non-alphanumeric with space \n",
        "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
        "    # replace codes with space\n",
        "    for i in range(len(codelist)):\n",
        "        stopstring = ' ' + codelist[i] + '  '\n",
        "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
        "    # replace single-character words with space\n",
        "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
        "    # convert uppercase to lowercase\n",
        "    temp_string = temp_string.lower()    \n",
        "    if REMOVE_STOPWORDS:\n",
        "        # replace selected character strings/stop-words with space\n",
        "        for i in range(len(stoplist)):\n",
        "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
        "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
        "    # replace multiple blank characters with one blank character\n",
        "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
        "    return(temp_string)\n",
        "\n",
        "def read_data(filename):\n",
        "\n",
        "  with open(filename) as f:\n",
        "    data = tf.compat.as_str(f.read())\n",
        "    data = data.lower()\n",
        "    data = text_parse(data)\n",
        "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
        "\n",
        "  return data\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0vOqd5GsQWuN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "REMOVE_STOPWORDS = False  # no stopword removal \n",
        "#EVOCABSIZE = 20000  # specify desired size of pre-defined embedding vocabulary\n",
        "EVOCABSIZE = 400000\n",
        "\n",
        "embeddings_directory = 'embeddings/glove.6B'\n",
        "filename = 'glove.6B.50d.txt'\n",
        "#embeddings_directory = 'embeddings/glove.twitter.27B'\n",
        "#filename = 'glove.twitter.27B.100d.txt'\n",
        "\n",
        "\n",
        "\n",
        "def simplify(embeddings_directory, filename):\n",
        "\tembeddings_filename = os.path.join(embeddings_directory, filename)\n",
        "\tprint('\\nLoading embeddings from', embeddings_filename)\n",
        "\tword_to_index, index_to_embedding = \\\n",
        "\t\tload_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
        "\tprint(\"Embedding loaded from disks.\")\n",
        "\n",
        "\tvocab_size, embedding_dim = index_to_embedding.shape\n",
        "\tprint(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
        "\tprint(\"This means (number of words, number of dimensions per word)\\n\")\n",
        "\tprint(\"The first words are words that tend occur more often.\")\n",
        "\n",
        "\tprint(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
        "\t\t  \"and the index is the last one. The dictionnary has a limit:\")\n",
        "\tprint(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
        "\t\t  \"Representation\"))\n",
        "\tword = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
        "\tidx = word_to_index[word] # index for word obviously not in the vocabulary\n",
        "\tcomplete_vocabulary_size = idx \n",
        "\tembd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
        "\tprint(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
        "\tword = \"the\"\n",
        "\tidx = word_to_index[word]\n",
        "\tembd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
        "\tprint(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
        "\n",
        "\ta_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
        "\tprint('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
        "\twords_in_test_sentence = a_typing_test_sentence.split()\n",
        "\n",
        "\tprint('Test sentence embeddings from complete vocabulary of', \n",
        "\t\t  complete_vocabulary_size, 'words:\\n')\n",
        "\tfor word in words_in_test_sentence:\n",
        "\t\tword_ = word.lower()\n",
        "\t\tembedding = index_to_embedding[word_to_index[word_]]\n",
        "\t\tprint(word_ + \": \", embedding)\n",
        "\n",
        "\tlimited_word_to_index = defaultdict(default_factory, \\\n",
        "\t\t{k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
        "\n",
        "\tlimited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
        "\tlimited_index_to_embedding = np.append(limited_index_to_embedding, \n",
        "\t\tindex_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
        "\t\t\treshape(1,embedding_dim), \n",
        "\t\taxis = 0)\n",
        "\n",
        "\tdel index_to_embedding\n",
        "\n",
        "\t# Verify the new vocabulary: should get same embeddings for test sentence\n",
        "\t# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
        "\tprint('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
        "\tfor word in words_in_test_sentence:\n",
        "\t\tword_ = word.lower()\n",
        "\t\tembedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
        "\t\tprint(word_ + \": \", embedding)\n",
        "\n",
        "\tcodelist = ['\\r', '\\n', '\\t']   \n",
        "\n",
        "\tif REMOVE_STOPWORDS:\n",
        "\t\tprint(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "\tmore_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
        "\t\t\t'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
        "\t\t\t've', 're', 'vs'] \n",
        "\n",
        "\tsome_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
        "\t\t\t'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
        "\t\t\t'toni','welles','william','wolheim','nikita']\n",
        "\n",
        "\tstoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
        "\t\t\tsome_proper_nouns_to_remove\n",
        "\n",
        "\tdir_name = \"/content/gdrive/My Drive/MSPA/Predict 422/Week 8/movie-reviews-negative\"\n",
        "\t#dir_name = '/root/movie-reviews-negative'\n",
        "\t\t\n",
        "\tfilenames = listdir_no_hidden(path=dir_name)\n",
        "\tnum_files = len(filenames)\n",
        "\n",
        "\tfor i in range(len(filenames)):\n",
        "\t\tfile_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
        "\t\tassert file_exists\n",
        "\tprint('\\nDirectory:',dir_name)    \n",
        "\tprint('%d files found' % len(filenames))\n",
        "\n",
        "\t# Read data for negative movie reviews\n",
        "\t# Data will be stored in a list of lists where the each list represents \n",
        "\t# a document and document is a list of words.\n",
        "\t# We then break the text into words.\n",
        "\n",
        "\tnegative_documents = []\n",
        "\n",
        "\tprint('\\nProcessing document files under', dir_name)\n",
        "\tfor i in range(num_files):\n",
        "\t\t## print(' ', filenames[i])\n",
        "\n",
        "\t\twords = read_data(os.path.join(dir_name, filenames[i]))\n",
        "\n",
        "\t\tnegative_documents.append(words)\n",
        "\t\t# print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
        "\t\t# print('Sample string (Document %d) %s'%(i,words[:50]))\n",
        "\n",
        "\t# -----------------------------------------------\n",
        "\t# gather data for 500 positive movie reviews\n",
        "\t# -----------------------------------------------\n",
        "\t#dir_name = '/root/movie-reviews-positive'\n",
        "\tdir_name = \"/content/gdrive/My Drive/MSPA/Predict 422/Week 8/movie-reviews-positive\"\n",
        "\tfilenames = listdir_no_hidden(path=dir_name)\n",
        "\tnum_files = len(filenames)\n",
        "\n",
        "\tfor i in range(len(filenames)):\n",
        "\t\tfile_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
        "\t\tassert file_exists\n",
        "\tprint('\\nDirectory:',dir_name)    \n",
        "\tprint('%d files found' % len(filenames))\n",
        "\n",
        "\t# Read data for positive movie reviews\n",
        "\t# Data will be stored in a list of lists where the each list \n",
        "\t# represents a document and document is a list of words.\n",
        "\t# We then break the text into words.\n",
        "\n",
        "\tpositive_documents = []\n",
        "\n",
        "\tprint('\\nProcessing document files under', dir_name)\n",
        "\tfor i in range(num_files):\n",
        "\t\t## print(' ', filenames[i])\n",
        "\n",
        "\t\twords = read_data(os.path.join(dir_name, filenames[i]))\n",
        "\n",
        "\t\tpositive_documents.append(words)\n",
        "\t\t# print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
        "\t\t# print('Sample string (Document %d) %s'%(i,words[:50]))\n",
        "\n",
        "\t# -----------------------------------------------------\n",
        "\t# convert positive/negative documents into numpy array\n",
        "\t# note that reviews vary from 22 to 1052 words   \n",
        "\t# so we use the first 20 and last 20 words of each review \n",
        "\t# as our word sequences for analysis\n",
        "\t# -----------------------------------------------------\n",
        "\tmax_review_length = 0  # initialize\n",
        "\tfor doc in negative_documents:\n",
        "\t\tmax_review_length = max(max_review_length, len(doc))    \n",
        "\tfor doc in positive_documents:\n",
        "\t\tmax_review_length = max(max_review_length, len(doc)) \n",
        "\tprint('max_review_length:', max_review_length) \n",
        "\n",
        "\tmin_review_length = max_review_length  # initialize\n",
        "\tfor doc in negative_documents:\n",
        "\t\tmin_review_length = min(min_review_length, len(doc))    \n",
        "\tfor doc in positive_documents:\n",
        "\t\tmin_review_length = min(min_review_length, len(doc)) \n",
        "\tprint('min_review_length:', min_review_length) \n",
        "\n",
        "\t# construct list of 1000 lists with 40 words in each list\n",
        "\tfrom itertools import chain\n",
        "\tdocuments = []\n",
        "\tfor doc in negative_documents:\n",
        "\t\tdoc_begin = doc[0:20]\n",
        "\t\tdoc_end = doc[len(doc) - 20: len(doc)]\n",
        "\t\tdocuments.append(list(chain(*[doc_begin, doc_end])))    \n",
        "\tfor doc in positive_documents:\n",
        "\t\tdoc_begin = doc[0:20]\n",
        "\t\tdoc_end = doc[len(doc) - 20: len(doc)]\n",
        "\t\tdocuments.append(list(chain(*[doc_begin, doc_end])))    \n",
        "\n",
        "\t# create list of lists of lists for embeddings\n",
        "\tembeddings = []    \n",
        "\tfor doc in documents:\n",
        "\t\tembedding = []\n",
        "\t\tfor word in doc:\n",
        "\t\t   embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
        "\t\tembeddings.append(embedding)\n",
        "\t\n",
        "\tembeddings_array = np.array(embeddings) \n",
        "\t\n",
        "\t# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
        "\tthumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
        "\t\t\t\t\t\t  np.ones((500), dtype = np.int32)), axis = 0)\n",
        "\n",
        "\n",
        "\t# Random splitting of the data in to training (80%) and test (20%)  \n",
        "\tX_train, X_test, y_train, y_test = \\\n",
        "\t\ttrain_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
        "\t\t\t\t\t\t random_state = RANDOM_SEED)\n",
        "\t\n",
        "\treturn (X_train, X_test, y_train, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}